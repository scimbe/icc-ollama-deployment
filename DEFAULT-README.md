# ICC Ollama Deployment

Automatisierte Bereitstellung von Ollama mit GPU-UnterstÃ¼tzung auf der HAW Hamburg Informatik Compute Cloud (ICC). Jetzt auch mit **RAG-UnterstÃ¼tzung** ohne Enterprise-Lizenzen!

## Ãœbersicht

Dieses Repository enthÃ¤lt Scripts und Konfigurationsdateien, um Ollama mit GPU-UnterstÃ¼tzung auf der ICC der HAW Hamburg zu deployen. ZusÃ¤tzlich wird ein Ollama WebUI als BenutzeroberflÃ¤che bereitgestellt. 

**NEU**: RAG-UnterstÃ¼tzung (Retrieval-Augmented Generation) mit lokalen Elasticsearch und Kibana Containern, ohne kostenpflichtige Enterprise-Lizenzen!

## Inhaltsverzeichnis

- [Voraussetzungen](#voraussetzungen)
- [ICC-Zugang einrichten](#icc-zugang-einrichten)
- [Schnellstart](#schnellstart)
- [Detaillierte Anleitung](#detaillierte-anleitung)
- [GPU-Ressourcen skalieren](#gpu-ressourcen-skalieren)
- [GPU-Testen und Ãœberwachen](#gpu-testen-und-Ã¼berwachen)
- [RAG-UnterstÃ¼tzung](#rag-unterstÃ¼tzung) ğŸ‘ˆ **NEU!**
- [Architektur](#architektur)
- [Troubleshooting](#troubleshooting)
- [Wartung](#wartung)
- [Lizenz](#lizenz)

## Voraussetzungen

- HAW Hamburg infw-Account mit Zugang zur ICC
- kubectl installiert
- Docker und Docker Compose (fÃ¼r RAG-FunktionalitÃ¤t)
- (Optional) Terraform installiert (Nur fÃ¼r das lokale WebUI-Deployment)
- Eine aktive VPN-Verbindung zum HAW-Netz (wenn auÃŸerhalb des HAW-Netzes)
- (Optional) Make installiert fÃ¼r vereinfachte Befehle

## ICC-Zugang einrichten

Bevor Sie beginnen kÃ¶nnen, mÃ¼ssen Sie sich bei der ICC anmelden und Ihre Kubeconfig-Datei einrichten. Dazu stellen wir ein Hilfsskript bereit:

```bash
# Ã–ffnet den Browser mit der ICC-Login-Seite und fÃ¼hrt Sie durch die Einrichtung
./scripts/icc-login.sh
```

Dieses Skript:
1. Ã–ffnet die ICC-Login-Seite in Ihrem Standard-Browser
2. FÃ¼hrt Sie durch den Anmeldeprozess mit Ihrer infw-Kennung
3. Hilft beim Speichern und Einrichten der heruntergeladenen Kubeconfig-Datei
4. Testet die Verbindung und zeigt Ihre Namespace-Informationen an

Alternativ kÃ¶nnen Sie die [manuelle Einrichtung](DOCUMENTATION.md#1-icc-zugang-einrichten) durchfÃ¼hren.

## Schnellstart

```bash
# Repository klonen
git clone <repository-url>
cd icc-ollama-deployment

# ICC-Zugang einrichten (falls noch nicht geschehen)
./scripts/icc-login.sh

# Konfiguration anpassen
cp configs/config.example.sh configs/config.sh
vim configs/config.sh  # Passen Sie Ihre Namespace-Informationen an

# AusfÃ¼hrungsberechtigungen fÃ¼r Skripte setzen
./scripts/set-permissions.sh

# Deployment ausfÃ¼hren
./deploy.sh
```

Oder mit Make:

```bash
make deploy
```

## Detaillierte Anleitung

Eine ausfÃ¼hrliche Schritt-fÃ¼r-Schritt-Anleitung finden Sie in der [DOCUMENTATION.md](DOCUMENTATION.md) Datei.

## GPU-Ressourcen skalieren

Um die Performance zu optimieren oder grÃ¶ÃŸere Modelle zu unterstÃ¼tzen, kÃ¶nnen Sie die Anzahl der GPUs dynamisch anpassen:

```bash
# Skalieren auf 2 GPUs fÃ¼r verbesserte Performance
./scripts/scale-gpu.sh --count 2

# Reduzieren auf 1 GPU, wenn nicht alle Ressourcen benÃ¶tigt werden
./scripts/scale-gpu.sh --count 1
```

Weitere Details zur GPU-Skalierung finden Sie in der [ausfÃ¼hrlichen Dokumentation](DOCUMENTATION.md#7-gpu-ressourcen-skalieren).

## GPU-Testen und Ãœberwachen

Das Projekt enthÃ¤lt verschiedene Skripte zum Testen, Ãœberwachen und Benchmarken der GPU-FunktionalitÃ¤t:

### GPU-FunktionalitÃ¤t testen

ÃœberprÃ¼fen Sie, ob die GPU korrekt eingerichtet ist und von Ollama genutzt wird:

```bash
./scripts/test-gpu.sh
# oder
make gpu-test
```

### GPU-Auslastung Ã¼berwachen

Ãœberwachen Sie die GPU-Auslastung in Echtzeit:

```bash
./scripts/monitor-gpu.sh
# oder
make gpu-monitor
```

Mit Optionen fÃ¼r kontinuierliche Ãœberwachung oder CSV-Export:

```bash
# 10 Messungen im 5-Sekunden-Intervall
./scripts/monitor-gpu.sh -i 5 
```

### GPU-Benchmarks durchfÃ¼hren

FÃ¼hren Sie Leistungstests fÃ¼r ein spezifisches Modell durch:

```bash
./scripts/benchmark-gpu.sh llama3:8b
# oder
make gpu-bench MODEL=llama3:8b
```

## RAG-UnterstÃ¼tzung

Neu hinzugefÃ¼gt: RAG-UnterstÃ¼tzung (Retrieval-Augmented Generation), die ohne Enterprise-Lizenzen auskommt!

### Was ist RAG?

RAG (Retrieval-Augmented Generation) verbindet LLMs mit externen Wissensdatenbanken. Vorteile:
- Reduzierte Halluzinationen durch Zugriff auf verifizierte Informationen
- Zugriff auf aktuellere Informationen als im Trainingskorpus
- MÃ¶glichkeit zur Quellenangabe

### Schnellstart RAG

```bash
# Bitte stellen Sie sicher, dass Ollama lÃ¤uft und port-forwarding aktiv ist:
# kubectl -n $NAMESPACE port-forward svc/$OLLAMA_SERVICE_NAME 11434:11434

# RAG-Infrastruktur lokal starten
./scripts/setup-rag.sh

# Test-Dokument hochladen
./scripts/upload-rag-documents.sh --direct rag/data/sample-document.md

# Ã–ffnen Sie http://localhost:3000 im Browser
```

### RAG-Komponenten

Die RAG-LÃ¶sung besteht aus folgenden Komponenten:

1. **Elasticsearch**: Speichert und indiziert Dokumente und Embeddings
2. **Kibana**: WeboberflÃ¤che fÃ¼r Elasticsearch zur Datenvisualisierung
3. **RAG-Gateway**: Vermittelt zwischen WebUI, Elasticsearch und Ollama
4. **Open WebUI**: Bleibt unverÃ¤ndert, kommuniziert aber mit dem Gateway

Alle Komponenten auÃŸer Ollama werden als lokale Docker-Container ausgefÃ¼hrt:

```
â”Œâ”€â”€â”€ Lokale Umgebung â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€ ICC Kubernetes Cluster â”€â”€â”€â”
â”‚                                         â”‚     â”‚                              â”‚
â”‚ Open WebUI â†’ RAG Gateway â†’ Elasticsearchâ”‚ â†â†’  â”‚ Ollama (mit GPU-Support)    â”‚
â”‚     â†‘                     â†‘             â”‚     â”‚                              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚     â”‚                              â”‚
â”‚                 â†‘                       â”‚     â”‚                              â”‚
â”‚               Kibana                    â”‚     â”‚                              â”‚
â”‚                                         â”‚     â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Keine Enterprise-Lizenzen erforderlich

Diese RAG-Implementierung benÃ¶tigt keine kostenpflichtigen Lizenzen:

- Verwendet die kostenlose Basic-Lizenz von Elasticsearch
- Implementiert ein einfaches, eigenes Embedding-Verfahren
- Funktioniert mit allen Ollama-Modellen

Weitere Details zur RAG-Integration finden Sie in der [RAG-Dokumentation](RAG-README.md).

## Architektur

Einen Ãœberblick Ã¼ber die Systemarchitektur und die Komponenten des Projekts finden Sie in der [ARCHITECTURE.md](ARCHITECTURE.md) Datei.

## Troubleshooting

### Ollama und GPU

Bei Problemen mit der GPU-FunktionalitÃ¤t kÃ¶nnen folgende Schritte helfen:

1. ÃœberprÃ¼fen Sie die GPU-KompatibilitÃ¤t: `make gpu-compat`
2. Testen Sie die GPU-FunktionalitÃ¤t: `make gpu-test`
3. ÃœberprÃ¼fen Sie die Deployment-Konfiguration: `kubectl -n $NAMESPACE get deployment $OLLAMA_DEPLOYMENT_NAME -o yaml`
4. PrÃ¼fen Sie die Logs des Ollama-Pods: `make logs`
5. Ã–ffnen Sie eine Shell im Pod: `make shell`

Weitere Informationen zur Fehlerbehebung finden Sie in der [DOCUMENTATION.md](DOCUMENTATION.md#8-fehlerbehebung).

### RAG-Komponenten

Bei Problemen mit der RAG-FunktionalitÃ¤t:

1. **WebUI verbindet nicht mit RAG-Gateway**:
   - PrÃ¼fen Sie, ob das RAG-Gateway lÃ¤uft: `docker ps | grep rag-gateway`
   - PrÃ¼fen Sie die Logs: `docker logs rag-gateway`

2. **Elasticsearch startet nicht**:
   - PrÃ¼fen Sie, ob genÃ¼gend Arbeitsspeicher verfÃ¼gbar ist
   - PrÃ¼fen Sie die Logs: `docker logs elasticsearch`

3. **Keine Dokumente gefunden**:
   - Stellen Sie sicher, dass Sie Dokumente hochgeladen haben
   - PrÃ¼fen Sie den Elasticsearch-Index in Kibana: http://localhost:5601

## Wartung

Die automatisierten Skripte erleichtern die Wartung des Systems:

- **GPU-Komponenten**: Nutzen Sie die GPU-Test- und Monitoring-Werkzeuge fÃ¼r kontinuierliche Ãœberwachung
- **RAG-Komponenten**: Die Docker-Container kÃ¶nnen einfach aktualisiert und neu gestartet werden
- **Ollama-Updates**: Aktualisieren Sie das Deployment mit dem neuesten Ollama-Image

## Lizenz

Dieses Projekt steht unter der [MIT-Lizenz](LICENSE).
