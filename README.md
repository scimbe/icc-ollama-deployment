# Ollama Installation und Dokumentation


Automatisierte Bereitstellung von Ollama mit GPU-Unterst√ºtzung und Modellanpassung auf der HAW Hamburg Informatik Compute Cloud (ICC).


- **[DEFAULT-README.md](DEFAULT-README.md)**: Beinhaltet die standardm√§√üige Installation und Nutzung von Ollama ohne RAG-Integration.

Dieses Repository enth√§lt Scripts und Konfigurationsdateien, um Ollama mit GPU-Unterst√ºtzung auf der ICC der HAW Hamburg zu deployen. Zus√§tzlich wird ein Ollama WebUI als Benutzeroberfl√§che bereitgestellt sowie Funktionen zur Anpassung der Modelle an spezifische Anwendungsf√§lle.

## Inhaltsverzeichnis

- [Voraussetzungen](#voraussetzungen)
- [ICC-Zugang einrichten](#icc-zugang-einrichten)
- [Schnellstart](#schnellstart)
- [Detaillierte Anleitung](#detaillierte-anleitung)
- [GPU-Ressourcen skalieren](#gpu-ressourcen-skalieren)
- [GPU-Testen und √úberwachen](#gpu-testen-und-√ºberwachen)
- [Modellanpassung und Finetuning](#modellanpassung-und-finetuning) üëà **NEU!**
- [Architektur](#architektur)
- [Troubleshooting](#troubleshooting)
- [Wartung](#wartung)
- [Lizenz](#lizenz)


## Voraussetzungen

- HAW Hamburg infw-Account mit Zugang zur ICC
- kubectl installiert
- (Optional) Terraform installiert (Nur f√ºr das lokale WebUI-Deployment)
- Eine aktive VPN-Verbindung zum HAW-Netz (wenn au√üerhalb des HAW-Netzes)
- (Optional) Make installiert f√ºr vereinfachte Befehle
- (Optional) IntelliJ IDEA f√ºr die erweiterte IDE-Integration

## ICC-Zugang einrichten

Bevor Sie beginnen k√∂nnen, m√ºssen Sie sich bei der ICC anmelden und Ihre Kubeconfig-Datei einrichten. Dazu stellen wir ein Hilfsskript bereit:

```bash
# √ñffnet den Browser mit der ICC-Login-Seite und f√ºhrt Sie durch die Einrichtung
./scripts/icc-login.sh
```

Dieses Skript f√ºhrt Sie durch den gesamten Prozess:
1. √ñffnet die ICC-Login-Seite in Ihrem Standard-Browser
2. F√ºhrt Sie durch den Anmeldeprozess mit Ihrer infw-Kennung
3. Hilft beim Speichern und Einrichten der heruntergeladenen Kubeconfig-Datei
4. Testet die Verbindung und zeigt Ihre Namespace-Informationen an

Alternativ k√∂nnen Sie die [manuelle Einrichtung](DOCUMENTATION.md#1-icc-zugang-einrichten) durchf√ºhren.

## Schnellstart

```bash
# Repository klonen
git clone <repository-url>
cd icc-ollama-deployment

# ICC-Zugang einrichten (falls noch nicht geschehen)
./scripts/icc-login.sh

# Konfiguration anpassen
cp configs/config.example.sh configs/config.sh
vim configs/config.sh  # Passen Sie Ihre Namespace-Informationen an

# Ausf√ºhrungsberechtigungen f√ºr Skripte setzen
./scripts/set-permissions.sh

# Deployment ausf√ºhren
./deploy.sh
```

Oder mit Make:

```bash
make deploy
```

## Detaillierte Anleitung

Eine ausf√ºhrliche Schritt-f√ºr-Schritt-Anleitung finden Sie in der [DOCUMENTATION.md](DOCUMENTATION.md) Datei.

## GPU-Ressourcen skalieren

Um die Performance zu optimieren oder gr√∂√üere Modelle zu unterst√ºtzen, k√∂nnen Sie die Anzahl der GPUs dynamisch anpassen:

```bash
# Skalieren auf 2 GPUs f√ºr verbesserte Performance
./scripts/scale-gpu.sh --count 2

# Reduzieren auf 1 GPU, wenn nicht alle Ressourcen ben√∂tigt werden
./scripts/scale-gpu.sh --count 1
```

Weitere Details zur GPU-Skalierung finden Sie in der [ausf√ºhrlichen Dokumentation](DOCUMENTATION.md#7-gpu-ressourcen-skalieren).

## GPU-Testen und √úberwachen

Das Projekt enth√§lt verschiedene Skripte zum Testen, √úberwachen und Benchmarken der GPU-Funktionalit√§t:

### GPU-Funktionalit√§t testen

√úberpr√ºfen Sie, ob die GPU korrekt eingerichtet ist und von Ollama genutzt wird:

```bash
./scripts/test-gpu.sh
# oder
make gpu-test
```

### GPU-Auslastung √ºberwachen

√úberwachen Sie die GPU-Auslastung in Echtzeit:

```bash
./scripts/monitor-gpu.sh
# oder
make gpu-monitor
```

Mit Optionen f√ºr kontinuierliche √úberwachung oder CSV-Export:

```bash
# 10 Messungen im 5-Sekunden-Intervall
./scripts/monitor-gpu.sh -i 5 
```

### GPU-Benchmarks durchf√ºhren

F√ºhren Sie Leistungstests f√ºr ein spezifisches Modell durch:

```bash
./scripts/benchmark-gpu.sh llama3:8b
# oder
make gpu-bench MODEL=llama3:8b
```

### GPU-Kompatibilit√§t pr√ºfen

√úberpr√ºfen Sie die vollst√§ndige GPU-Konfiguration und -Kompatibilit√§t:

```bash
./scripts/check-gpu-compatibility.sh
# oder
make gpu-compat
```

## Modellanpassung und Finetuning

Das Projekt bietet M√∂glichkeiten zur Anpassung von Modellen f√ºr spezifische Anwendungsf√§lle:

### Modell anpassen (Finetuning)

Passen Sie ein Modell an Ihre spezifischen Anforderungen an:

```bash
./scripts/finetune-simple.sh -m llama3:8b -n haw-custom -d examples/haw_training_data.jsonl
# oder
make finetune-simple MODEL=llama3:8b NAME=haw-custom DATA=examples/haw_training_data.jsonl
```

Das Skript f√ºhrt folgende Schritte aus:
1. Erstellt ein angepasstes Modell mit HAW-spezifischem Template
2. Trainiert es mit den angegebenen Trainingsdaten
3. Macht es als neues Modell verf√ºgbar

### Trainingsdaten vorbereiten

Konvertieren Sie JSONL-Trainingsdaten in verschiedene Formate:

```bash
./scripts/convert-training-data.sh -i my_data.jsonl -o converted_data.txt -f txt
# oder
make convert-training-data INPUT=my_data.jsonl FORMAT=ollama
```

Unterst√ºtzte Formate:
- `txt`: Einfaches Textformat mit Frage-Antwort-Struktur
- `md`: Markdown-Format f√ºr bessere Lesbarkeit
- `ollama`: Format optimiert f√ºr Ollama-Training

### Templates erstellen

Erstellen Sie benutzerdefinierte Modelfile-Templates f√ºr verschiedene Anwendungsf√§lle:

```bash
./scripts/create-template.sh -t academic -l de my_template
# oder
make create-template TYPE=academic NAME=my_template
```

Template-Typen:
- `academic`: F√ºr wissenschaftliche/akademische Anwendungen
- `chat`: F√ºr konversationelle Assistenten
- `coding`: Optimiert f√ºr Programmierunterst√ºtzung
- `assistance`: Allgemeiner Assistenten-Modus

### Angepasste Modelle testen

Testen Sie Ihre angepassten Modelle mit verschiedenen Prompts:

```bash
./scripts/test-model.sh haw-custom
# oder
make test-model MODEL=haw-custom
```

F√ºr Batch-Tests mit mehreren Prompts:
```bash
./scripts/test-model.sh -b prompts.txt haw-custom
```

Weitere Details zur Modellanpassung finden Sie in der [ausf√ºhrlichen Dokumentation](DOCUMENTATION.md#11-modellanpassung-und-finetuning).

## Architektur

Einen √úberblick √ºber die Systemarchitektur und die Komponenten des Projekts finden Sie in der [ARCHITECTURE.md](ARCHITECTURE.md) Datei.

## Troubleshooting

Bei Problemen mit der GPU-Funktionalit√§t oder Modellanpassung k√∂nnen folgende Schritte helfen:

1. √úberpr√ºfen Sie die GPU-Kompatibilit√§t: `make gpu-compat`
2. Testen Sie die GPU-Funktionalit√§t: `make gpu-test`
3. √úberpr√ºfen Sie die Deployment-Konfiguration: `kubectl -n $NAMESPACE get deployment $OLLAMA_DEPLOYMENT_NAME -o yaml`
4. Pr√ºfen Sie die Logs des Ollama-Pods: `make logs`
5. √ñffnen Sie eine Shell im Pod: `make shell`

Weitere Informationen zur Fehlerbehebung finden Sie in der [DOCUMENTATION.md](DOCUMENTATION.md#9-fehlerbehebung).

## Wartung

Die Funktionen f√ºr GPU-Tests, Monitoring und Modellanpassung erm√∂glichen ein kontinuierliches Management Ihrer Ollama-Instanz, um sicherzustellen, dass sie optimal mit den verf√ºgbaren Ressourcen arbeitet und an Ihre spezifischen Anforderungen angepasst ist.

